import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

np.random.seed(70)

# Generamos datos para dos clases
n_samples = 800  # Total de muestras
n_features = 2     # Dos características para visualización fácil

# Generamos datos para la clase 0
n_samples_class0 = n_samples // 2
mean0 = [1, 1]
cov0 = [[2, 0], [0, 2]]  # Matriz de covarianza

X0 = np.random.multivariate_normal(mean0, cov0, n_samples_class0)

# Generamos datos para la clase 1
n_samples_class1 = n_samples - n_samples_class0
mean1 = [8, 8]
cov1 = [[1, 0], [0, 1]]

X1 = np.random.multivariate_normal(mean1, cov1, n_samples_class1)

# Combinamos los datos y las etiquetas
X = np.vstack((X0, X1))
y = np.hstack((np.zeros(n_samples_class0), np.ones(n_samples_class1)))

# Mezclamos los datos
shuffle_indices = np.random.permutation(n_samples)
X = X[shuffle_indices]
y = y[shuffle_indices]

# Dividimos los datos en conjunto de entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y)

# Graficamos el scatter plot de los datos
plt.figure(figsize=(8, 6))
plt.scatter(X0[:, 0], X0[:, 1], alpha=0.5, label='Clase 0')
plt.scatter(X1[:, 0], X1[:, 1], alpha=0.5, label='Clase 1')
plt.title('Distribución de las dos clases')
plt.xlabel('Característica 1')
plt.ylabel('Característica 2')
plt.legend()
plt.show()





