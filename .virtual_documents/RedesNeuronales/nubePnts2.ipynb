





# Importación de paquetes
import matplotlib.pyplot as plt
import numpy as np
import sklearn
import sklearn.datasets
import sklearn.linear_model
import matplotlib
import seaborn as sns  # Añadido para graficar la matriz de confusión
from sklearn.metrics import roc_curve, auc, confusion_matrix, classification_report

# Mostrar gráficos en línea y cambiar el tamaño de figura predeterminado
get_ipython().run_line_magic("matplotlib", " inline")
matplotlib.rcParams['figure.figsize'] = (10.0, 8.0)





import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from sklearn.model_selection import train_test_split

# Semilla para reproducibilidad
np.random.seed(0)

# Generar las dos nubes de puntos con make_blobs
X, y = make_blobs(n_samples=1000, centers=[[3, 3], [6, 6]], cluster_std=0.8, random_state=0)

# Dividir en conjuntos de entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)

# Graficar los datos de entrenamiento
plt.scatter(X_train[:, 0], X_train[:, 1], s=40, c=y_train, cmap=plt.cm.Spectral)
plt.title('Datos de Entrenamiento')
plt.xlabel('X1')
plt.ylabel('X2')
plt.show()






# Función auxiliar para graficar una frontera de decisión
def plot_decision_boundary(pred_func, X, y):
    # Establecer valores mínimos y máximos y darle algo de relleno
    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5
    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5
    h = 0.01
    # Generar una cuadrícula de puntos con distancia h entre ellos
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    # Predecir el valor de la función para toda la cuadrícula
    Z = pred_func(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    # Graficar el contorno y los ejemplos
    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)
    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral)









































# Parámetros de la red y del descenso de gradiente
nn_input_dim = X_train.shape[1]  # Dimensionalidad de la capa de entrada
nn_output_dim = 2  # Dimensionalidad de la capa de salida

# Parámetros de descenso de gradiente
epsilon = 0.01  # Tasa de aprendizaje para el descenso de gradiente
reg_lambda = 0.01  # Fuerza de regularización





# Función auxiliar para evaluar la pérdida total en el conjunto de datos
def calculate_loss(model, X, y):
    num_examples = len(X)
    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']
    # Propagación hacia adelante para calcular nuestras predicciones
    z1 = X.dot(W1) + b1
    a1 = np.tanh(z1)
    z2 = a1.dot(W2) + b2
    exp_scores = np.exp(z2)
    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)
    # Calculando la pérdida
    correct_logprobs = -np.log(probs[range(num_examples), y])
    data_loss = np.sum(correct_logprobs)
    # Agregar término de regularización a la pérdida (opcional)
    data_loss += reg_lambda/2 * (np.sum(np.square(W1)) + np.sum(np.square(W2)))
    return 1./num_examples * data_loss





# Función auxiliar para predecir probabilidades
def predict_probs(model, x):
    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']
    # Propagación hacia adelante
    z1 = x.dot(W1) + b1
    a1 = np.tanh(z1)
    z2 = a1.dot(W2) + b2
    exp_scores = np.exp(z2)
    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)
    return probs

# Función auxiliar para predecir una salida (0 o 1)
def predict(model, x):
    probs = predict_probs(model, x)
    return np.argmax(probs, axis=1)





# Función para graficar la curva ROC
def plot_roc_curve(fpr, tpr, roc_auc, dataset_type):
    plt.plot(fpr, tpr, label='Curva ROC (área = %0.2f)' % roc_auc)
    plt.plot([0,1], [0,1], 'k--')
    plt.xlim([-0.05,1.0])
    plt.ylim([0.0,1.05])
    plt.xlabel('Tasa de Falsos Positivos')
    plt.ylabel('Tasa de Verdaderos Positivos')
    plt.title('Curva ROC (' + dataset_type + ')')
    plt.legend(loc="lower right")

# Función para graficar la matriz de confusión
def plot_confusion_matrix(cm, dataset_type):
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title('Matriz de Confusión (' + dataset_type + ')')
    plt.ylabel('Etiqueta Real')
    plt.xlabel('Etiqueta Predicha')





# Función del modelo de Red Neuronal
def build_model(X, y, nn_hdim, num_passes=20000, print_loss=False):
    num_examples = len(X)
    nn_input_dim = X.shape[1]
    nn_output_dim = 2  # Número de clases

    # Parámetros de descenso de gradiente
    epsilon = 0.01  # Tasa de aprendizaje
    reg_lambda = 0.01  # Fuerza de regularización

    # Inicializar los parámetros a valores aleatorios
    np.random.seed(0)
    W1 = np.random.randn(nn_input_dim, nn_hdim) / np.sqrt(nn_input_dim)
    b1 = np.zeros((1, nn_hdim))
    W2 = np.random.randn(nn_hdim, nn_output_dim) / np.sqrt(nn_hdim)
    b2 = np.zeros((1, nn_output_dim))

    # Esto es lo que devolvemos al final
    model = {}

    # Descenso de gradiente
    for i in range(0, num_passes):

        # Propagación hacia adelante
        z1 = X.dot(W1) + b1
        a1 = np.tanh(z1)
        z2 = a1.dot(W2) + b2
        exp_scores = np.exp(z2)
        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)

        # Retropropagación
        delta3 = probs
        delta3[range(num_examples), y] -= 1
        dW2 = (a1.T).dot(delta3)
        db2 = np.sum(delta3, axis=0, keepdims=True)
        delta2 = delta3.dot(W2.T) * (1 - np.power(a1, 2))
        dW1 = np.dot(X.T, delta2)
        db1 = np.sum(delta2, axis=0)

        # Agregar términos de regularización
        dW2 += reg_lambda * W2
        dW1 += reg_lambda * W1

        # Actualización de parámetros por descenso de gradiente
        W1 += -epsilon * dW1
        b1 += -epsilon * db1
        W2 += -epsilon * dW2
        b2 += -epsilon * db2

        # Asignar nuevos parámetros al modelo
        model = {'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}

        # Opcionalmente imprimir la pérdida
        if print_loss and i % 1000 == 0:
            loss = calculate_loss(model, X, y)
            print(f"Pérdida después de la iteración {i}: {loss}")

    return model





hidden_layer_dimensions = [1, 3, 5, 20, 50]

for nn_hdim in hidden_layer_dimensions:
    print('Tamaño de la Capa Oculta %d' % nn_hdim)
    # Entrenar el modelo
    model = build_model(X_train, y_train, nn_hdim)

    # Calcular probabilidades en los conjuntos de entrenamiento y prueba
    probs_train = predict_probs(model, X_train)
    probs_test = predict_probs(model, X_test)

    # Calcular clases predichas
    y_pred_train = np.argmax(probs_train, axis=1)
    y_pred_test = np.argmax(probs_test, axis=1)

    # Calcular curvas ROC y AUC
    fpr_train, tpr_train, _ = roc_curve(y_train, probs_train[:, 1])
    roc_auc_train = auc(fpr_train, tpr_train)

    fpr_test, tpr_test, _ = roc_curve(y_test, probs_test[:, 1])
    roc_auc_test = auc(fpr_test, tpr_test)

    # Calcular matrices de confusión
    cm_train = confusion_matrix(y_train, y_pred_train)
    cm_test = confusion_matrix(y_test, y_pred_test)

    # Graficar
    # Crear una figura con 3 filas y 2 columnas
    fig, axes = plt.subplots(3, 2, figsize=(12, 18))
    fig.suptitle('Tamaño de la Capa Oculta %d' % nn_hdim, fontsize=16)

    # Primera fila: Fronteras de decisión
    plt.sca(axes[0, 0])
    plot_decision_boundary(lambda x: predict(model, x), X_train, y_train)
    plt.title('Frontera de Decisión (Entrenamiento)')

    plt.sca(axes[0, 1])
    plot_decision_boundary(lambda x: predict(model, x), X_test, y_test)
    plt.title('Frontera de Decisión (Prueba)')

    # Segunda fila: Curvas ROC
    plt.sca(axes[1, 0])
    plot_roc_curve(fpr_train, tpr_train, roc_auc_train, 'Entrenamiento')

    plt.sca(axes[1, 1])
    plot_roc_curve(fpr_test, tpr_test, roc_auc_test, 'Prueba')

    # Tercera fila: Matrices de confusión
    plt.sca(axes[2, 0])
    plot_confusion_matrix(cm_train, 'Entrenamiento')

    plt.sca(axes[2, 1])
    plot_confusion_matrix(cm_test, 'Prueba')

    plt.tight_layout()
    plt.subplots_adjust(top=0.92)
    plt.show()



